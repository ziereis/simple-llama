{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "torch.set_default_device('cpu')\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "from model import ModelArgs, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### state dict contains the following weights:\n",
    "\n",
    "#### Input\n",
    "* tok_embeddings (vocab_size, embedding_dim) = (32000,4096)\n",
    "### Layer 0-31\n",
    "#### Attention\n",
    "* attention_norm (embedding_dim) = (4096)\n",
    "* attention.wq (embedding_dim, embedding_dim) = (4096, 4096)\n",
    "* attention.wk (embedding_dim, embedding_dim) = (4096, 4096)\n",
    "* attention.wv (embedding_dim, embedding_dim) = (4096, 4096)\n",
    "* attention.w0 (embedding_dim, embedding_dim) = (4096, 4096)\n",
    "#### FeedFordward\n",
    "* feed_forward.norm (embedding_dim) = (4096)\n",
    "* feed_forward.w1 (embedding_dim, hidden_dim) = (4096, 11008)\n",
    "* feed_forward.w3 (embedding_dim, hidden_dim) = (4096, 11008)\n",
    "\n",
    "(w1 and w3 get both applied to the input embeddings and then element wise multiplied)\n",
    "* feed_forward.w2 (hidden_dim, embedding_dim) = (11008, 4096)\n",
    "\n",
    "\n",
    "\n",
    "## Output\n",
    "* norm (embedding_dim) = (4096)\n",
    "* output (embedding_dim, vocab_size) = (4096, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama(checkpoints_dir: str, vocab_size: int, max_seq_len: int):\n",
    "    prev_time = time.time()\n",
    "    checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
    "    assert len(checkpoints) > 0, f\"no checkpoint files found in {checkpoints_dir}\"\n",
    "    ckpt_path = checkpoints[0]\n",
    "    print(f'Loading checkpoint \"{ckpt_path}\"')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    print(f\"Loaded checkpoint in {time.time() - prev_time:.2f}s\")\n",
    "\n",
    "    with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "        print(f\"params: {params}\")\n",
    "\n",
    "    model_args = ModelArgs()\n",
    "    model_args.max_seq_len = max_seq_len\n",
    "\n",
    "    assert(model_args.dim == params['dim'])\n",
    "    assert(model_args.n_layers == params['n_layers'])\n",
    "    assert(model_args.vocab_size == vocab_size)\n",
    "    assert(model_args.n_heads == params['n_heads'])\n",
    "    assert(model_args.n_layers == params['n_layers'])\n",
    "\n",
    "    model_args.vocab_size = vocab_size\n",
    "    print(f\"model_args: {model_args}\")\n",
    "    model = Transformer(model_args)\n",
    "\n",
    "    del checkpoint['rope.freqs']\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "    print(f\"Loaded model in {time.time() - prev_time:.2f}s\")\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_path: str):\n",
    "    tokenizer = SentencePieceProcessor()\n",
    "    tokenizer.load(tokenizer_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint \"llama-2-7b/consolidated.00.pth\"\n",
      "Loaded checkpoint in 10.01s\n",
      "params: {'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-05, 'vocab_size': -1}\n",
      "model_args: ModelArgs(dim=4096, n_layers=32, n_heads=32, head_dim=128, hidden_dim=11008, vocab_size=32000, norm_eps=1e-05, max_seq_len=1024)\n",
      "Loaded model in 43.96s\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(\"tokenizer.model\")\n",
    "llama = load_llama(\"llama-2-7b\", tokenizer.vocab_size(), max_seq_len=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: Transformer, tokenizer: SentencePieceProcessor, promt: str, max_toks: int = 100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = tokenizer.encode(promt)\n",
    "        output = []\n",
    "        # feed the entire prompt as context\n",
    "        for token in tqdm(input, desc=\"feeding prompt\"):\n",
    "            out = model(token, len(output))\n",
    "            output.append(token)\n",
    "\n",
    "        # generate the rest of the tokens\n",
    "        for _ in tqdm(range(max_toks - len(output)), desc=\"generating\"):\n",
    "            out = model(output[-1], len(output))\n",
    "            probs = torch.softmax(out, dim=-1)\n",
    "            next_token = torch.argmax(probs, dim=-1).item()\n",
    "            if (next_token == tokenizer.eos_id()):\n",
    "                break\n",
    "            output.append(next_token)\n",
    "    return tokenizer.decode(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feeding prompt: 100%|██████████| 12/12 [00:35<00:00,  2.93s/it]\n",
      "generating: 100%|██████████| 38/38 [01:52<00:00,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that 1) the speed of light is the same for all observers, regardless of their relative motion, and 2) the laws of physics are the same for all observers, regardless of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = generate(llama, tokenizer, \"Simply put, the theory of relativity states that \", max_toks=50)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
