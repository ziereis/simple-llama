# TODO
# create some looping code here to:
# - comparison cpu vs gpu
# - run 4bit, 8bit, 32bit
# - compare llama.cpp vs. own implementation
# - compare token/s, firsttoken_duration, avg_token_time
# - compare output mit augenma√ü
# - compare different inference strats